# the NN which computes the parameters of the elementwise transformations is a residual network
# with pre-activation residual blocks [19].
# For autoregressive transformations, we use the ResMADE architecture.

# ResMADE usese ReLU

# We fix the tail bound B = 3
# We fix the number of bins K = 8

# We implement all invertible linear transformations using the LU-decomposition,
# where the permutation matrix P is fixed at the beginning of training, and the
# product LU is initialized to the identity.
# we define a flow ‘step’ as the composition of an invertible linear transformation
# with either a coupling or autoregressive transform, and we use 10 steps per flow
# All flows use a standard-normal noise distribution.
# We use the Adam optimizer [27], and anneal the learning rate according to a cosine schedule
# In some cases, we find applying dropout [53] in the residual blocks beneficial for regularization.

# Code is available online at https://github.com/bayesiains/nsf

# Table 4: Hyperparameters for density-estimation results using coupling layers in section 5.1
# |                   |     POWER |     GAS | HEPMASS | MINIBOONE |   BSDS300 |
# |-------------------+-----------+---------+---------+-----------+-----------|
# | Dimension         |         6 |       8 |      21 |        43 |        63 |
# | Train data points | 1,615,917 | 852,174 | 315,123 |    29,556 | 1,000,000 |
# | Batch size        |       512 |     512 |     256 |       128 |       512 |
# | Training steps    |   400,000 | 400,000 | 400,000 |   200,000 |   400,000 |
# | Learning rate     |    0.0005 |  0.0005 |  0.0005 |    0.0003 |    0.0005 |
# | Flow steps        |        10 |      10 |      20 |        10 |        20 |
# | Residual blocks   |         2 |       2 |       2 |         1 |         1 |
# | Hidden features   |       256 |     256 |     128 |        32 |       128 |
# | Bins              |         8 |       8 |       8 |         4 |         8 |
# | Dropout           |       0.0 |     0.1 |     0.2 |       0.2 |       0.2 |

# Table 5: Hyperparameters for density-estimation results using autoregressive layers in section 5.1
# |                   |     POWER |     GAS | HEPMASS | MINIBOONE |   BSDS300 |
# |-------------------+-----------+---------+---------+-----------+-----------|
# | Dimension         |         6 |       8 |      21 |        43 |        63 |
# | Train data points | 1,615,917 | 852,174 | 315,123 |    29,556 | 1,000,000 |
# | Batch size        |       512 |     512 |      64 |       128 |       512 |
# | Training steps    |   400,000 | 400,000 | 400,000 |   250,000 |   400,000 |
# | Learning rate     |    0.0005 |  0.0005 |  0.0005 |    0.0003 |    0.0005 |
# | Flow steps        |        10 |      10 |      10 |        10 |        10 |
# | Residual blocks   |         2 |       2 |       2 |         1 |         2 |
# | Hidden features   |       256 |     256 |      64 |        64 |       512 |
# | Bins              |         8 |       8 |       8 |         4 |         8 |
# | Dropout           |       0.0 |     0.1 |     0.2 |       0.2 |       0.2 |

compile_kwargs: {}
#   jit_compile: true
fit_kwargs:
  batch_size: 128
  epochs: &epochs 200
  learning_rate:
    scheduler_kwargs:
      decay_steps: *epochs
      initial_learning_rate: 0.0003
    scheduler_name: cosine_decay
  monitor: val_loss
  reduce_lr_on_plateau: false
  early_stopping: 30
  verbose: true
model_kwargs:
  distribution: coupling_flow
  num_layers: 10
  permutation: 1x1conv
  random_permutation_seed: 1
  bijector: RationalQuadraticSpline
  bijector_kwargs:
    range_min: -3
  parameters_constraint_fn_kwargs:
    interval_width: 6
    min_slope: 0.001
    min_bin_width: 0.001
    nbins: 4
  num_parameters: 11 # nbins * 3 - 1
  parameters_fn: fully_connected_res_net
  parameters_fn_kwargs:
    activation: relu
    batch_norm: false
    dropout: 0.2
    res_blocks: 1
    hidden_features: 32
