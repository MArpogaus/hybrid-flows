#+TITLE: Zwischenstand
#+DATE: <2024-03-22 Fri>
#+STARTUP: latexpreview

* Übersicht Modelle
| Models                       | sim | benchmark | malnutrition |
|------------------------------+-----+-----------+--------------|
| multivariate Normal          | IS  |           |              |
| MCTM                         | IS  |           |              |
| BNF (elementwise)            | IS  | I         |              |
| NSF (elementwise)            |     |           |              |
| Gaussian Copula              |     |           |              |
| Vine copula                  |     |           |              |
| realNVP                      |     |           |              |
| coupling NSF                 | IS  |           |              |
| coupling BNF                 | IS  |           |              |
| MAF                          |     |           |              |
| IAF                          |     |           |              |
| MAF BNF                      | IS  | I         |              |
| MAF NSF                      | IS  |           |              |
| Hybrid MAF BNF               | I   |           |              |
| Hybrid MAF NSF               | I   |           |              |
| Hybrid (pre-trained) MAF BNF | I   | I         |              |
| Hybrid (pre-trained) MAF NSF | I   | I         |              |

- I :: Implemented
- S :: Satisfactory results
- O :: HP optimized

* Aktueller Stand

- Probleme mit =tfp= konnten gelöst werden -> Fokus weiterhin auf bestehende =tf= Implementierung.
- =PyTorch= Implementierung (=zuko=) für autoregressiven (MAF) Bernstein Flow steht. Bisher kein hybrides Modell.
- Große Teile des Codes wurden Überarbeitet und erweitert.
- Performance Probleme / Unterschiede verstanden.

* Flexibility is not enough

Diskussion mit Zuko Entwickler: https://github.com/probabilists/zuko/issues/36#issuecomment-1995122776

#+begin_quote
A single auto-regressive transformation should be enough if the uni-variate transformation is an universal (monotonic) function approximator.
However, the hyper network (MADE/MaskedMLP) conditioning the transformation must also be a universal function approximator.
In practice, the capacity of the hyper network is finite and the function it should approximate might be complex depending on the data, the uni-variate function (and its parametrization) and the auto-regressive order.
The latter, in particular, can have a huge impact: some orders can lead to very simple functions while others can lead to almost unlearnable functions.
Stacking several multi-variate transformations with different (sometimes randomized) orders help to alleviate this issue.

For example if $p(x_1,x_2,\ldots,x_n)=p(x_1)\prod p(x_i|x1)$ (star-shaped dependency graph), the order $1 \to n$ leads to a simple function, while the order $n \to 1$ does not. This is somewhat related to finding the Bayesian network (a directed acyclic graph) with the least number of edges that explains the data. In fact, the "simplest" Bayesian network is sometimes confused with the causal graph of the data.
#+end_quote

* Hyper Parameters of other polynomial Flows
** Ramasinghe et al. 2022: Robust Normalizing Flows Using Bernstein-type Polynomials [cite:@Ramasinghe2022]
#+begin_quote
We observed that a single layer model with 100 degree polynomials performed well for the real-world data.
In contrast, for 2D toy distributions and and images we used higher number of layers (8) with 15 degree polynomials in each layer.
#+end_quote

- fully-connected neural net with three layers to obtain the parameters (no MADE).
- The weights are initialized randomly using a standard normal distribution.
- SGD with Adam, learning rate 0.01, decay factor of 10% per 50 iterations.
- Kumaraswamy distribution with parameters $a = 2$ and $b = 5$ as the base density.
- Using a standard normal distribution after converting it to a density on $[0, 1]$ using a nonlinear transformation, e.g., $1+\tanh(z)$ , also yielded similar results.

** Durkan et al. 2019: Neural Spline Flows [cite:@Durkan2019b]
#+begin_quote
We modify MAF by replacing permutations with invertible linear layers.
Hyperparameter settings are shown for coupling flows in
[[experimental-details-density-estimation-coupling]]
and autoregressive flows in
[[experimental-details-density-estimation-autoregressive]].
We include the dimensionality and number of training data points in each
table for reference. For higher dimensional datasets such as Hepmass and
BSDS300, we found increasing the number of coupling layers beneficial.
This was not necessary for Miniboone, where overfitting was an issue due
to the low number of data points.
#+end_quote

#+CAPTION: Hyperparameters for density-estimation results using coupling layers
#+NAME: experimental-details-density-estimation-coupling
|                   |     Power |     Gas | Hepmass | Miniboone |   BSDS300 |
|-------------------+-----------+---------+---------+-----------+-----------|
| Dimension         |         6 |       8 |      21 |        43 |        63 |
| Train data points | 1,615,917 | 852,174 | 315,123 |    29,556 | 1,000,000 |
| Batch size        |       512 |     512 |     256 |       128 |       512 |
| Training steps    |   400,000 | 400,000 | 400,000 |   200,000 |   400,000 |
| Learning rate     |    0.0005 |  0.0005 |  0.0005 |    0.0003 |    0.0005 |
| Flow steps        |        10 |      10 |      20 |        10 |        20 |
| Residual blocks   |         2 |       2 |       1 |         1 |         1 |
| Hidden features   |       256 |     256 |     128 |        32 |       128 |
| Bins              |         8 |       8 |       8 |         4 |         8 |
| Dropout           |       0.0 |     0.1 |     0.2 |       0.2 |       0.2 |

#+CAPTION: Hyperparameters for density-estimation results using autoregressive layers in
#+NAME: experimental-details-density-estimation-autoregressive
|                   |     Power |     Gas | Hepmass | Miniboone |   BSDS300 |
|-------------------+-----------+---------+---------+-----------+-----------|
| Dimension         |         6 |       8 |      21 |        43 |        63 |
| Train data points | 1,615,917 | 852,174 | 315,123 |    29,556 | 1,000,000 |
| Batch size        |       512 |     512 |     512 |        64 |       512 |
| Training steps    |   400,000 | 400,000 | 400,000 |   250,000 |   400,000 |
| Learning rate     |    0.0005 |  0.0005 |  0.0005 |    0.0003 |    0.0005 |
| Flow steps        |        10 |      10 |      10 |        10 |        10 |
| Residual blocks   |         2 |       2 |       2 |         1 |         2 |
| Hidden features   |       256 |     256 |     256 |        64 |       512 |
| Bins              |         8 |       8 |       8 |         4 |         8 |
| Dropout           |       0.0 |     0.1 |     0.2 |       0.2 |       0.2 |

#+CAPTION: Validation log likelihood (in nats) for UCI datasets and BSDS300, with error bars corresponding to two standard deviations.
| Model       | POWER           | GAS              | HEPMASS           | MINIBOONE                      | BSDS300           |
|-------------+-----------------+------------------+-------------------+--------------------------------+-------------------|
| RQ-NSF (C)  | \(0.65 \pm 0.01\) | \(13.08 \pm 0.02\) | \(-14.75 \pm 0.06\) | \(\hphantom{0}{-9.03} \pm 0.43\) | \(172.51 \pm 0.60\) |
| RQ-NSF (AR) | \(0.67 \pm 0.01\) | \(13.08 \pm 0.02\) | \(-13.82 \pm 0.05\) | \(\hphantom{0}{-8.63} \pm 0.41\) | \(172.5 \pm 0.59\)  |

#+begin_quote
For autoregressive transformations, the layers must be masked so as to preserve autoregressive structure, and so we use the ResMADE architecture outlined by Nash and Durkan [cite:@Nash2019]
#+end_quote
*** Nash et al. 2019: Autoregressive Energy Machines [cite:@Nash2019]

#+CAPTION: ResMADE architecture with $ D = 3 $ input data dimensions and $ H = 4 $ hidden units. The degree of each hidden unit and output is indicated with an integer label. Sequential degree assignment results in each hidden layer having the same masking structure, here alternating between dependence on the first input, or the first two inputs. These layers can be combined using any binary elementwise operation, while preserving autoregressive structure. In particular, residual connections can be added in a straightforward manner.The ResMADE architecture consists of an initial masked projection to the target hidden dimensionality, a sequence of masked residual blocks, and finally a masked linear layer to the output units.
[[file:./gfx/ResMADE.png]]

** Jaini et al. 2019:  Sum-of-Squares Polynomial Flow [cite:@Jaini2019]
#+begin_quote
The SOS transformation was trained using maximum likelihood method with source density as standard normal distribution.
#+end_quote

 - conditioner network implemented as in MAF [cite:@Papamakarios2018].
 - We used stochastic gradient descent with
   - batch size :: 1000
   - learning rate :: 0.001
   - number of stacked blocks :: 8
   - number of polynomials (k) :: 5
   - degree of polynomials (r) :: 4
   - with number of epochs for training :: 40
* Aktuelle Hyperparameter
** Masked autoregressive Bernstein Flow

#+begin_src yaml
    distribution: masked_autoregressive_flow
    distribution_kwargs:
      allow_flexible_bounds: false
      bijector_name: bernstein_poly
      bounds: linear
      high: 4
      low: -4
      scale_to_domain: true
      num_layers: 4
      order: 32
    # parameter_fn: autoregressive_res_net
    parameter_kwargs:
      activation: sigmoid
      hidden_units:
      - 16
      - 16
      - 16
#+end_src
** Masked autoregressive Spline Flow

#+begin_src yaml
    distribution: masked_autoregressive_flow
    distribution_kwargs:
      bijector_name: quadratic_spline
      nbins: 8
      interval_width: 8
      range_min: -4
      min_bin_width: 0.001
      min_slope: 0.001
      num_layers: 10
    # get_parameter_fn: autoregressive_res_net
    parameter_kwargs:
      activation: sigmoid
      hidden_units:
      - 16
      - 16
      # res_blocks: 2
      # res_block_units: 16
      # batch_norm: true
      # dropout: 0.1
#+end_src

* Aktuelle Probleme

** Bisherige Ergebnisse auf Benchmark Daten
| Model       | Power       | Gas          | Hepmass        | MiniBoone      | BSDS300       |
|-------------+-------------+--------------+----------------+----------------+---------------|
| MAF         | 0.45 ± 0.01 | 12.35 ± 0.02 | − 17.03 ± 0.02 | − 10.92 ± 0.46 | 156.95 ± 0.28 |
| RQ-NSF (AR) | 0.66 ± 0.01 | 13.09 ± 0.02 | − 14.01 ± 0.03 | − 9.22 ± 0.48  | 157.31 ± 0.28 |
| Q-NSF (AR)  | 0.66 ± 0.01 | 13.09 ± 0.02 | − 14.01 ± 0.03 | − 9.22 ± 0.48  | 157.31 ± 0.28 |
| SOS         | 0.60 ± 0.01 | 11.99 ± 0.41 | − 15.15 ± 0.10 | − 8.90 ± 0.11  | 157.48 ± 0.41 |
| BERNSTEIN   | 0.63 ± 0.01 | 12.81 ± 0.01 | − 15.11 ± 0.02 | − 8.93 ± 0.08  | 157.13 ± 0.11 |
| MABNF       | − 0.006     | −            | − 16.62        | − 12.32        | 161.4         |
** Ergebnisse mit stacked flow
| Model       | Power       | Gas          | Hepmass        | MiniBoone      | BSDS300       |
|-------------+-------------+--------------+----------------+----------------+---------------|
| MABNF       | −0.5        | 8.6          | −-             | − 12.82        | --            |
** Schlechte Regularization
[[file:gfx/copula_surface.pdf]]
* Zusammenarbeit mit Philipp Baumann
- Unterstützung bei Programmierung nuere Flows (Unconstrained Monotonic Neural Networks (UMNN))
- TS density regression
  - CRPS
  - Energy Score
* Nächste Schritte / To-dos

** TODO ResMADE
** TODO Unconstrained Monotonic Neural Networks (UMNN) [cite:@Wehenkel2021]
** TODO HPO für stacked Flows auf benchmark Daten
** TODO [#B] Interpretirbarkeit auf Covariaten (sim-moons, TS, malnutrition)
** PROJ [#B] Vergleich mit MCTM, Gaussin Copula
