#+STARTUP: latexpreview indent content
* Unstructured
** Idee Implemnetierung
*** Univariate Models
**** Unconditional univariate Models
#+begin_src python
  dist_lambda=get_dist_lambda(**kwds)
  pv_shape = M * dims + np.sum(np.arange(dims + 1))
  pv = tf.Variable(tf.random.normal([pv_shape], dtype=tf.float32), trainable=True)
  dist = dist_lambda(pv)
  nll = - dist.log_prob(y)
#+end_src
**** Conditional univariate Models
#+begin_src python
  dist_lambda=get_dist_lambda(**kwds)
  pv_shape = M * dims + np.sum(np.arange(dims + 1))
  conditioner = get_conditioner(pv_shape, cond_shape)
  pv_cond = conditioner(x)
  dist = dist_lambda(pv_cond)
  nll = - dist.log_prob(y)
#+end_src
*** Multivariate Models
**** Unconditional multivariate models
#+begin_src python
  network = ar_cond_network(params, dims,...)
  bijector_fn = get_bijector_fn(
      network=network
  )
  # maybe stack multiple
  dist = tfd.TransformedDistribution(
      distribution=distribution,
      bijector=ARFlow(bijector_fn=bijector_fn),
  )
  nll = - dist.log_prob(y)
#+end_src
**** Conditional multivariate models
#+begin_src python
  network = ar_cond_network(params,dims,..., conditional=True, cond_shape, ...)
  bijector_fn = get_bijector_fn(
      network=network
  )
  # maybe stack multiple
  dist = tfd.TransformedDistribution(
      distribution=distribution,
      bijector=ARFlow(bijector_fn=bijector_fn),
  )
  nll = - dist.log_prob(y, conditional_input=x)
#+end_src
** Implementation
#+begin_src python
  # unconditional case:
  dist, variables = mctm.distributions.unconditional.get_dist(name, dims, kwds...)
  model = get_unconditional_model(dist, varibales)
  hist = fit_distribution(model, data, ...)

  # conditional case:
  dist, variables = mctm.distributions.conditional.get_dist(name, dims, cond_shape, kwds...)
  model = get_unconditional_model(dist, varibales)
  hist = fit_distribution(model, data, ...)
#+end_src

** Models

| Models                       | sim | benchmark | malnutrition |
|------------------------------+-----+-----------+--------------|
| multivariate Normal          | I   |           |              |
| MCTM                         | I   |           |              |
| BNF (elementwise)            | I   | I         |              |
| NSF (elementwise)            |     |           |              |
| Gaussian Copula              |     |           |              |
| Vine copula                  |     |           |              |
| realNVP                      |     |           |              |
| coupling NSF                 |     |           |              |
| coupling BNF                 | I   |           |              |
| MAF                          |     |           |              |
| IAF                          |     |           |              |
| MAF BNF                      | I   | I         |              |
| MAF NSF                      |     |           |              |
| Hybrid MAF BNF               | I   | I         |              |
| Hybrid MAF NSF               |     |           |              |
| Hybrid (pre-trained) MAF BNF |     |           |              |
| Hybrid (pre-trained) MAF NSF |     |           |              |

- I :: Implemented
- S :: Satisfactory results
- O :: HP optimized

** Flexibility is not enough
https://github.com/probabilists/zuko/issues/36#issuecomment-1995122776

#+begin_quote
A single auto-regressive transformation should be enough if the uni-variate transformation is an universal (monotonic) function approximator.
However, the hyper network (MADE/MaskedMLP) conditioning the transformation must also be a universal function approximator.
In practice, the capacity of the hyper network is finite and the function it should approximate might be complex depending on the data, the uni-variate function (and its parametrization) and the auto-regressive order.
The latter, in particular, can have a huge impact: some orders can lead to very simple functions while others can lead to almost unlearnable functions.
Stacking several multi-variate transformations with different (sometimes randomized) orders help to alleviate this issue.

For example if $p(x_1,x_2,\ldots,x_n)=p(x_1)\prod p(x_i|x1)$ (star-shaped dependency graph), the order $1 \to n$ leads to a simple function, while the order $n \to 1$ does not. This is somewhat related to finding the Bayesian network (a directed acyclic graph) with the least number of edges that explains the data. In fact, the "simplest" Bayesian network is sometimes confused with the causal graph of the data.
#+end_quote

** Hyper Parameters of other polynomial Flows

*** Ramasinghe et al. 2022: Robust Normalizing Flows Using Bernstein-type Polynomials [cite:@Ramasinghe2022]
#+begin_quote
For each $B^n_j$ , we employ a fully-connected neural net with three layers to obtain the parameters, except in the case of $B^0_n$ in which we directly optimize the parameters.

We use maximum likelihood to train the model with a learning rate 0.01 with a decay factor of 10% per 50 iterations.
All the weights are initialized randomly using a standard normal distribution.

For optimization, we used the Adam optimizer with parameters β1 = 0.9, β1 = 0.999, ε =
1 × 10−8 , where parameters refer to the usual notation.

We observed that a single layer model with 100 degree polynomials performed well for the real-world data.

In contrast, for 2D toy distributions and and images we used higher number of layers (8) with 15 degree polynomials in each layer.

For all the experiments, we use a Kumaraswamy distribution with parameters $a = 2$ and $b = 5$ as the base density. Using a standard normal distribution after converting it to a density on $[0, 1]$ using a nonlinear transformation, e.g., $1+\tanh(z)$ , also yielded similar results.
#+end_quote

*** Durkan et al. 2019: Neural Spline Flows [cite:@Durkan2019b]
#+begin_quote
We modify MAF by replacing permutations with invertible linear layers.
Hyperparameter settings are shown for coupling flows in
[[#tab:experimental-details-density-estimation-coupling][[tab:experimental-details-density-estimation-coupling]]]
and autoregressive flows in
[[#tab:experimental-details-density-estimation-autoregressive][[tab:experimental-details-density-estimation-autoregressive]]].
We include the dimensionality and number of training data points in each
table for reference. For higher dimensional datasets such as Hepmass and
BSDS300, we found increasing the number of coupling layers beneficial.
This was not necessary for Miniboone, where overfitting was an issue due
to the low number of data points.
#+end_quote

#+CAPTION: Hyperparameters for density-estimation results using coupling layers
#+TBLNAME: experimental-details-density-estimation-coupling
|                   | Power     | Gas     | Hepmass | Miniboone | BSDS300   |
|-------------------+-----------+---------+---------+-----------+-----------|
| Dimension         | 6         | 8       | 21      | 43        | 63        |
| Train data points | 1,615,917 | 852,174 | 315,123 | 29,556    | 1,000,000 |
| Batch size        | 512       | 512     | 256     | 128       | 512       |
| Training steps    | 400,000   | 400,000 | 400,000 | 200,000   | 400,000   |
| Learning rate     | 0.0005    | 0.0005  | 0.0005  | 0.0003    | 0.0005    |
| Flow steps        | 10        | 10      | 20      | 10        | 20        |
| Residual blocks   | 2         | 2       | 1       | 1         | 1         |
| Hidden features   | 256       | 256     | 128     | 32        | 128       |
| Bins              | 8         | 8       | 8       | 4         | 8         |
| Dropout           | 0.0       | 0.1     | 0.2     | 0.2       | 0.2       |

#+CAPTION: Hyperparameters for density-estimation results using autoregressive layers in
#+TBLNAME: experimental-details-density-estimation-autoregressive
|                   | Power     | Gas     | Hepmass | Miniboone | BSDS300   |
|-------------------+-----------+---------+---------+-----------+-----------|
| Dimension         | 6         | 8       | 21      | 43        | 63        |
| Train data points | 1,615,917 | 852,174 | 315,123 | 29,556    | 1,000,000 |
| Batch size        | 512       | 512     | 512     | 64        | 512       |
| Training steps    | 400,000   | 400,000 | 400,000 | 250,000   | 400,000   |
| Learning rate     | 0.0005    | 0.0005  | 0.0005  | 0.0003    | 0.0005    |
| Flow steps        | 10        | 10      | 10      | 10        | 10        |
| Residual blocks   | 2         | 2       | 2       | 1         | 2         |
| Hidden features   | 256       | 256     | 256     | 64        | 512       |
| Bins              | 8         | 8       | 8       | 4         | 8         |
| Dropout           | 0.0       | 0.1     | 0.2     | 0.2       | 0.2       |

#+CAPTION: Validation log likelihood (in nats) for UCI datasets and BSDS300, with error bars corresponding to two standard deviations.
| Model       | POWER           | GAS              | HEPMASS           | MINIBOONE                      | BSDS300           |
|-------------+-----------------+------------------+-------------------+--------------------------------+-------------------|
| RQ-NSF (C)  | \(0.65 \pm 0.01\) | \(13.08 \pm 0.02\) | \(-14.75 \pm 0.06\) | \(\hphantom{0}{-9.03} \pm 0.43\) | \(172.51 \pm 0.60\) |
| RQ-NSF (AR) | \(0.67 \pm 0.01\) | \(13.08 \pm 0.02\) | \(-13.82 \pm 0.05\) | \(\hphantom{0}{-8.63} \pm 0.41\) | \(172.5 \pm 0.59\)  |

#+begin_quote
For autoregressive
transformations, the layers must be masked so as to preserve autoregressive structure, and so we use
the ResMADE architecture outlined by Nash and Durkan [cite:@Nash2019]
#+end_quote

**** Nash et al. 2019: Autoregressive Energy Machines [cite:@Nash2019]
#+CAPTION: ResMADE architecture with $ D = 3 $ input data dimensions and $ H = 4 $ hidden units. The degree of each hidden unit and output is indicated with an integer label. Sequential degree assignment results in each hidden layer having the same masking structure, here alternating between dependence on the first input, or the first two inputs. These layers can be combined using any binary elementwise operation, while preserving autoregressive structure. In particular, residual connections can be added in a straightforward manner.The ResMADE architecture consists of an initial masked projection to the target hidden dimensionality, a sequence of masked residual blocks, and finally a masked linear layer to the output units.
[[file:gfx/ResMADE.png][ResMade Arcitecture]]

*** Jaini et al. 2019:  Sum-of-Squares Polynomial Flow [cite:@Jaini2019]
#+begin_quote
...we implement the conditioner network in the same way as in [cite:@Papamakarios2018].

The SOS transformation was trained using maximum likelihood method with source density as standard normal distribution.

We used stochastic gradient descent to train our models with
- a batch size of 1000,
- learning rate = 0.001,
- number of stacked blocks = 8,
- number of polynomials (k) = 5 and,
- degree of polynomials (r) = 4
- with number of epochs for training = 40.
#+end_quote

** Debug DAPE

*** conditional@coupling_flow-moons
#+begin_src emacs-lisp
  debugpy :cwd "/home/work/Projekte/MCTM/code/" :program "scripts/train.py" :args ["--log-level" "info" "--experiment-name" "unconditional-hybrid-pre-trained-moons" "unconditional@coupling_flow-moons" "coupling_flow" "moons" "/tmp" "--test-mode=true"]
#+end_src

*** conditional@elementwise_flow-moons

#+begin_src emacs-lisp
  debugpy :cwd "/home/work/Projekte/MCTM/code/" :program "scripts/train.py" :args ["--log-level" "info" "--experiment-name" "conditional-moons" "conditional@elemntwise_flow-moons" "elementwise_flow" "moons" "/tmp" "--test-mode=true"]
#+end_src

** Generic Normalizing Flow implementation
*** Element-wise
#+begin_src python
  model = DenistyRegressionModel(
      dims = 3,
      distribution = "normalizing_flow",
      transformations = [
          {
              "name": "scale",
              "parameters" [3],
          },
          {
              "name": "shift",
              "parameter_fn": "polynomial",
              "parameter_fn_kwargs": {
                  "order": 3
              }
          },
          {
              "name": "bernstein_poly",
              "parameter_fn": "fully_connected_net",
              "parameter_kwargs": {
                  ...,
                  conditional=True
              },
              "parameter_constrains":{
                  "type": "function",
                  "name": "softplus",
                  ""
              }
          }
      ]
  )
#+end_src
*** Multi Variate
#+begin_src python
  model = DenistyRegressionModel(
      dims = 3,
      distribution = "normalizing_flow",
      transformations = {
          "bernstein_poly" : {
              "parameter_fn": "coupling_flow",
              "parameter_kwargs": {
                  ...
              }
          }
      }
  )
#+end_src
* Meeting
** TK, TN, DR <2024-05-31 Fri 13:00>
*** Paper
- Inhalt passt erst mal, muss aber stark gekürzt werden
- Detailgrad abhängig von finaler Zielgruppe
- Notation fehlerhaft, aber erst mal in Ordnung
- Weitere Fokus auf detaillierte Ausarbeitung der Malnutrition Ergebnisse
*** Malnutrition als nächsten Schwerpunkt
- Joint Density auf Manutrition Daten schätzen
- Vergleich mit MCTM Paper
- Ergebnisse gerne vorab per Mail an alle
- PDF plots statt CDF
