compile_kwargs:
  jit_compile: false
fit_kwargs:
  batch_size: 128
  early_stopping: 100
  epochs: 200
  learning_rate:
    scheduler_kwargs:
      decay_steps: 200
      end_learning_rate: 0.0001
      initial_learning_rate: 0.0005
      max_learning_rate: 0.01
      stationary: 50
      warmup: 5
      warmup_power: 1
    scheduler_name: polynomial_warmup_and_cosine_decay
  lr_patience:
  monitor: val_loss
  reduce_lr_on_plateau: false
  verbose: true
model_kwargs:
  distribution: masked_autoregressive_flow
  distribution_kwargs:
    allow_flexible_bounds: true
    bijector_name: bernstein_poly
    bounds: linear
    high: 1
    low: 0
    # scale_to_domain: true
    num_layers: 10
    order: 8
  parameter_fn: autoregressive_res_net
  parameter_kwargs:
    activation: sigmoid
    hidden_units:
    - 128
    - 128
    res_blocks: 2
    res_block_units: 16
    batch_norm: true
    dropout: 0.1
